{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 - Implement Calibration (Spinup, Calibrate, Validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder From Lesson 2:\n",
    "We covered the creation of the database and initialization of job/experiment. After initialization, each Domain specified in the `Domain_Meta` table is now populated as a subdirectory named by the `gageID` listed in table `Domain_Meta`, at the location specified in `setup.parm` file. Each Domain Directory contains the following subdirectories:\n",
    "\n",
    "| SubDirectory Name | Description |\n",
    "| ------------- | ------------- |\n",
    "| FORCING | A symbolic link to the forcing directory for this particular basin |\n",
    "| OBS | The directory containing symbolic links to the observation files necessary for the calibration workflow |\n",
    "| RUN.CALIB | The directory that contains output for the calibration iterations. |\n",
    "|RUN.SPINUP|The directory that contains output for the calibration spinup. |\n",
    "|RUN.VALID|The directory that contains output for the calibration validation.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "ls /home/docker/example_case/Calibration/output/example1/01447720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spinup\n",
    "After successfully initializing model, the next step is to run a spinup period for each of the domains. This is handled by the script: `spinOrchestrator.py`. \n",
    "All the user spinup settings have been previously set in `setup.parm` file. \n",
    "\n",
    "|Argument|Required|Description|\n",
    "|-|-|-|\n",
    "|**coldStart** | Req. |Flag to direct calibration workflow to cold start your model simulation for each iteration during calibration. Specify 1 to activate. |\n",
    "|**optSpinFlag** | Req. | Flag to direct the workflow to use an alternative spinup file already in place in the input directory for the basins being calibrated. This allows the user to bypass the spinup step. Specify 1 to activate. |\n",
    "|**bSpinDate** | Opt. |Beginning date for the spinup. |\n",
    "|**eSpinDate** | Opt. |Ending date for the spinup. |\n",
    "\n",
    "**NOTE**: if optSpinFlag is set to 1, user must provide BOTH a hydro and land spinup state in the basin domain directories for ALL basins being used in this experiment. Expected file naming conventions for the expected restart files are as follows:\n",
    "* Land:  LandRestartSubstitute.nc\n",
    "* Hydro: HydroRestartSubstitute.nc\n",
    "\n",
    "To initiate model spinup, run the following on the commandline (or in the cell provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python /home/docker/PyWrfHydroCalib/spinOrchestrator.py 1 --optDbPath /home/docker/example_case/Calibration/output/DATABASE.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: ALL basins in a given job/experiment must complete their spinup for the program to complete successfully. The spinup program must complete before moving onto the parameter calibration. If you navigate to the `RUN.SPINUP` subdirectory under your basin, you will see an `OUTPUT` subdirectory. All files necessary to run the model for your spinup have been constructed here. This includes the namelist files, links to parameter files and executables, along with the spinup output. Let us check the content of the directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# navigate to spinup directory here, and show the OUTPUT file location\n",
    "ls /home/docker/example_case/Calibration/output/example1/01447720/RUN.SPINUP/OUTPUT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for any reason (like not having the right WRF-Hydro executable, or an incompatable namelist options) the spin up model runs fails, the workflow will create a lock file called `RUN.LOCK` file which will be on the `OUTPUT` directory. If you have specified the `email` argument in the setup file, then the workflow will send out an email to user and specify where the lock file is located (not set up in this training image). User needs to checkout the run directory (`OUTPUT`), identify why the model run failed, address the issue and then remove the `RUN.LOCK` file. After removing the lock file, the python workflow (`spinOrchestrator.py`) will restart from it was left and restart the model run. When spin up for all the basins in a given job/experiment is finished it will send an email to user with the following content: \n",
    "\n",
    "*SPINUP FOR JOB ID: 1 COMPLETE.*\n",
    "\n",
    "Upon receiving this email, user could move to next step and start the calibration process. Note, if you submit the calibration before spin up is finished, it will throw an error that the spin up has not yet finished and you cannot submit the calibration. \n",
    "\n",
    "## Calibration:\n",
    "You are now ready to calibrate your model parameters for your basins. To do this, we will run `calibOrchestrator.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens:\n",
    "The workflow will proceed to manage running each calibration iteration, which includes launching model jobs, adjusting parameter files used by the model, running analysis code to calculate new parameter values, and generating updated plots. The workflow will be monitoring each stage of the iteration to ensure things complete successfully. In order for the calibration to complete successfully, the workflow must complete all iterations for every basin in your experiment. \n",
    "\n",
    "The process of launching the calibration step is the same as the spinup step. \n",
    "Navigate to your calibration python code directory and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python /home/docker/PyWrfHydroCalib/calibOrchestrator.py 1 --optDbPath /home/docker/example_case/Calibration/output/DATABASE.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: JobID is the unique job ID value created when you initialized your experiment. The calibration program must complete before you can run the validation program.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Now lets view output files resulting from the Calibration step:\n",
    "\n",
    "# navigate to spinup directory here, and show the OUTPUT file location\n",
    "ls /home/docker/example_case/Calibration/output/example1/01447720/RUN.CALIB/OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes made by calibOrchestratory.py to RUN.CALIB directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Run.Calib Subdirectories|Description|\n",
    "|-|-|\n",
    "|calib_parm.tbl|copy of calibration parameter table specified during jobInit.py|\n",
    "|Multiple*|symbolic links to executables for R/Python/executables|\n",
    "|calibScript.R|a temporary R namelist file that is updated by the workflow in-between model iterations|\n",
    "|proj_data.Rdata|File created by R code to hold model statistics and parameter values as they are updated throughout the calibration process.|\n",
    "|BASELINE_PARAMETERS|directory contains the original parameter files specified during the domain input process. These parameter files include the groundwater parameter file, the 2D Hydro file, along with the Fulldom and soil properties file|\n",
    "|DEFAULT_PARAMETERS|The same as BASELINE, but with the default values applied to them from the calibration parameters table|\n",
    "|FINAL_PARAMETERS|directory contains the same adjusted files, but with the final calibrated parameter values applied to them.|\n",
    "|plots|This directory contains several PNG plots that can be very useful. A more detailed explanation of some of the products in this sub-directory are explained in lesson 4 to follow.|\n",
    "|OUTPUT|sub-directory containing model output for the last iteration. These files are very dynamic and get overwritten during each model iteration that is run|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes to Database made by calibOrchestratory.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Database Table|Description of Change|\n",
    "|-|-|\n",
    "|Calib_Params | Tables are updated dynamically by the workflow as the calibration progresses. The ‘Calib_Params’ table will contain the parameter values for each model iteration.|\n",
    "|Calib_Stats | Table contains error metrics for each model iteration as parameter values are adjusted. Many of these values are visualized through the plots that are generated during the calibration process. However, having these values in the database allows you to perform further analysis with the data after the fact to better understand how the system evolved over time.|\n",
    "\n",
    "\n",
    "When calibration for all the basins in a given job/experiment is finished it will send an email to user with the following content:\n",
    "\n",
    "CALIBRATION FOR JOB ID: 1 COMPLETE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "You are now ready to validate your calibrated parameter values for your basins via `validOrchestrator.py` . The process of running the validation is exactly the same as with the spinup and calibration.\n",
    "\n",
    "In your terminal session, or in the cell below, execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python /home/docker/PyWrfHydroCalib/runValidOrchestrator.py  /home/docker/PyWrfHydroCalib/ 1 --optDbPath /home/docker/example_case/Calibration/output/DATABASE.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: JobID is the unique job ID value created when you initialized your experiment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Now lets view output files resulting from the Valibration step:\n",
    "ls /home/docker/example_case/Calibration/output/example1/01447720/RUN.VALID/OUTPUT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the RUN.VALID/OUTPUT directory, there are two subdirectories that contain the restart and model run information for the default (CTRL) and Best (BEST) performing simulation runs. Contents of each shown below: \n",
    "\n",
    "* RUN.VALID/OUTPUT/BEST --> contains the restart and model run information for the best performing parameterization\n",
    "\n",
    "* RUN.VALID/OUTPUT/CTRL --> contains the restart and model run with default paramter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Now lets view output files resulting from the Valibration step:\n",
    "ls /home/docker/example_case/Calibration/output/example1/01447720/RUN.VALID/OUTPUT/*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens: \n",
    "\n",
    "The workflow will then proceed to manage running NWM/WRF-Hydro over your basins with the default parameter values specified at the beginning of the experiment, along with your calibrated parameter values. \n",
    "\n",
    "Under the `RUN.VALID` subdirectory for each basin, you will see several files and sub-directories as you did with the `RUN.CALIB` directory. \n",
    "\n",
    "## Conclusion:\n",
    "Now all the steps are complete, we will take a look at the results and plots next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
